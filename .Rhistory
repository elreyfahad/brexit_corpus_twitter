M[,-3]
p=norm(20,2,4)
p=rnorm(20,2,4)
plot(p)
setwd('D:\COURS\Master\S2\Introduction modilisation similation\Tp2019')
setwd("D:\COURS\Master\S2\Introduction modilisation similation\Tp2019")
setwd("..\D:\COURS\Master\S2\Introduction modilisation similation\Tp2019")
y1=(x1-1)/sqrt(2)
y1=(x-1)/sqrt(2)
y1=(x1-1)/sqrt(2)
x1=sqrt(-2*log(u))*cos(2*pi*v)
y2=(x2*sqrt(2))+1
n=10000
u=runif(n/2)
v=runif(n/2)
x1=sqrt(-2*log(u))*cos(2*pi*v)
x2=sqrt(-2*log(u))*sin(2*pi*v)
y1=(x1*sqrt(2))+1
y2=(x2*sqrt(2))+1
y=c(y1,y2)
box_cartesien=function(){
n=10000
u=runif(n/2)
v=runif(n/2)
x1=sqrt(-2*log(u))*cos(2*pi*v)
x2=sqrt(-2*log(u))*sin(2*pi*v)
y1=(x1*sqrt(2))+1
y2=(x2*sqrt(2))+1
y=c(y1,y2)
}
box_cartesien()
x=box_cartesien()
x
x=box_cartesien()
2^2
2.^2
y=box_polaire()
return(y)
box_pol=function(){
n=10000
u=runif(n/2)
v=runif(n/2)
s=(u.^2)*(v.^2)
while(s>1){
u=runif(n/2)
v=runif(n/2)
s=(u.^2)*(v.^2)
}
x1=u*sqrt((-2*log(s))/s)
x2=v*sqrt((-2*log(s))/s)
y1=(x1*sqrt(2))+1
y2=(x2*sqrt(2))+1
y=c(y1,y2)
return(y)
}
s=(u.^2)*(v.^2)
while(s>1){
u=runif(n/2)
v=runif(n/2)
s=(u.^2)*(v.^2)
}
x1=u*sqrt((-2*log(s))/s)
x2=v*sqrt((-2*log(s))/s)
y1=(x1*sqrt(2))+1
y2=(x2*sqrt(2))+1
y=c(y1,y2)
return(y)
box_polaire=function(){
n=10000
u=runif(n/2)
v=runif(n/2)
s=(u.^2)*(v.^2)
while(s>1){
u=runif(n/2)
v=runif(n/2)
s=(u.^2)*(v.^2)
}
x1=u*sqrt((-2*log(s))/s)
x2=v*sqrt((-2*log(s))/s)
y1=(x1*sqrt(2))+1
y2=(x2*sqrt(2))+1
y=c(y1,y2)
return(y)
}
y=box_polaire()
s=(u^2)*(v^2)
while(s>1){
u=runif(n/2)
v=runif(n/2)
s=(u.^2)*(v.^2)
}
s=(u^2)*(v^2)
while(s>1){
u=runif(n/2)
v=runif(n/2)
s=(u^2)*(v^2)
}
x1=u*sqrt((-2*log(s))/s)
x2=v*sqrt((-2*log(s))/s)
y1=(x1*sqrt(2))+1
y2=(x2*sqrt(2))+1
y=c(y1,y2)
return(y)
y=box_polaire()
s=(u*u)*(v*v)
while(s>1){
u=runif(n/2)
v=runif(n/2)
s=(u^2)*(v^2)
}
y=box_polaire()
scan()
scan()
scan(nmax = 1)
scan()
library("NLP")
library("twitteR")
library("tm")
library("SnowballC")
library("stringi")
library("topicmodels")
library("ROAuth")
library("wordcloud")
consumer_key <- 'r1nTeZaXHwfjXKARyaZIVurc9'
consumer_secret <- 'EikApVxGDxiybnugl0hDfpPhtuloCjDO43yAJMoXlfY3BBnDya'
access_token <- '2598381610-6d4pf6GUToA7Ib7UYndDOAotJWJqci45FQzG4B6'
access_secret <- 'UnOChQMyMsZLbZuqAA6XEy8dPdfSckDOO3OB3eMbjzMlG'
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
tweets_got <- searchTwitter("#GameOfThrones", n=1000,lang = "fr")
got_tweets <- twListToDF(tweets_got)
View(got_tweets)
got_text<-got_tweets$text
setwd("D:/TwitterCorpus")
library("NLP")
library("twitteR")
#le package "tm" propose un certain nombre de fonctions intéressantes pour le retravail de données textuelles
library("tm")
library("SnowballC")
library("stringi")
library("topicmodels")
library("ROAuth")
library("wordcloud")
consumer_key <- 'r1nTeZaXHwfjXKARyaZIVurc9'
consumer_secret <- 'EikApVxGDxiybnugl0hDfpPhtuloCjDO43yAJMoXlfY3BBnDya'
access_token <- '2598381610-GXnmHSk6RjqTCM3g795sSCWoU2ZobSgEDHZlEGG'
access_secret <- 'WTcf5ryAS7G0PFgWei9W06jHuYRbSKJSGPRv3knZOY0iS'
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
tweets_got <- searchTwitter("#Brexit", n=5000,lang = "en")
#suppression de Retweets
tweets_got = strip_retweets(tweets_got)
#cette fonction va permettre de transformer la liste de tweets extraite en un "dataframe"
got_tweets <- twListToDF(tweets_got)
str(got_tweets) #description du Dataframe
View(got_tweets) #visualisation du dataframe
got_text<-got_tweets$text #recuperation du text des tweets
tweets_text <- stri_trans_general(got_text, "lower") # on va ici mettre tous les mots en minuscules
myCleaningFunction <- function(x, mots) {
s <- x
#regex pour supprimer les liens https qui se trouvent dans les text
s <- gsub("http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+", "", s, perl = TRUE)
#suppression des points
s <- gsub("[\\.]*","",s,perl=TRUE)
#suppression des mentions
s <- gsub("@[a-zA-Z0-9]*","",s,perl=TRUE)
#suppression de certains mots qu'on a passe en parametre
for(k in 1:length(mots)) {
s <- gsub(mots[k], "", s, perl = TRUE)
}
return(s)
}
# vecteur de mots à supprimer,on va supprimer les stopword en anglais
mots_supp <- stopwords("en")
tweets_text <- sapply(tweets_text, myCleaningFunction, mots_supp)
names(tweets_text) <- NULL
donne=data.frame( id=got_tweets$id,tweet=tweets_text,
date=got_tweets$created,username=got_tweets$screenName)
View(donne) #visualisation du dataframe
write.csv2(donne, file = "brexit_tweets.csv",append = TRUE, row.names = FALSE)
#Creation d'un corpus a partir de la colonne "tweet" (le text des tweet) du dataframe "donne"
tweets_corpus <- Corpus(VectorSource(donne$tweet))
# ici cela va supprimer automatiquement tous les caractères de ponctuation
tweets_corpus <- tm_map(tweets_corpus, removePunctuation)
# ici cela va supprimer automatiquement une bonne partie des mots anglais basiques
tweets_corpus <- tm_map(tweets_corpus, removeWords, stopwords("en"))
# ici cela va supprimer automatiquement tous les espaces vides
tweets_corpus <- tm_map(tweets_corpus, stripWhitespace)
tweets_text<-tweets_corpus
dtm <- TermDocumentMatrix(tweets_corpus)
m <- as.matrix(dtm)
v <- sort(rowSums(m), decreasing = TRUE)
d <- data.frame(word = names(v),freq = v)
head(d, 20)
barplot(d[1:20,]$freq, las = 2, names.arg = d[1:20,]$word,
col ="lightblue", main ="Mots les plus fréquents",
ylab = "Fréquences")
set.seed(123456)
wordcloud(tweets_corpus, max.words = 200, colors = brewer.pal(8, "Dark2"))
myCleaningFunction <- function(x, mots) {
s <- x
#regex pour supprimer les liens https qui se trouvent dans les text
s <- gsub("http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+", "", s, perl = TRUE)
s <- gsub("[\\.]*","",s,perl=TRUE)
for(k in 1:length(mots)) {
s <- gsub(mots[k], "", s, perl = TRUE)
s <- gsub("[\w]*","",s,perl=TRUE)
s <- gsub("[' ']+","",s,perl=TRUE)
}
return(s)
}
myCleaningFunction <- function(x, mots) {
s <- x
#regex pour supprimer les liens https qui se trouvent dans les text
s <- gsub("http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+", "", s, perl = TRUE)
s <- gsub("[\\.]*","",s,perl=TRUE)
for(k in 1:length(mots)) {
s <- gsub(mots[k], "", s, perl = TRUE)
s <- gsub("[\\w]*","",s,perl=TRUE)
s <- gsub("[' ']+","",s,perl=TRUE)
}
return(s)
}
tweets_text <- sapply("hello @fahad @mohameed     tu va bien <>@ .. fahad .", myCleaningFunction, mots_supp)
names(tweets_text) <- NULL
tweets_text
myCleaningFunction <- function(x, mots) {
s <- x
#regex pour supprimer les liens https qui se trouvent dans les text
s <- gsub("http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+", "", s, perl = TRUE)
s <- gsub("[\\.]*","",s,perl=TRUE)
for(k in 1:length(mots)) {
s <- gsub(mots[k], "", s, perl = TRUE)
s <- gsub("[\\W]+","",s,perl=TRUE)
}
return(s)
}
# vecteur de mots à supprimer
mots_supp <- c("thrones", "game", "#gameofthrones","got","#of","episode")
tweets_text <- sapply("hello @fahad @mohameed     tu va bien <>@ .. fahad .", myCleaningFunction, mots_supp)
names(tweets_text) <- NULL
tweets_text
myCleaningFunction <- function(x, mots) {
s <- x
#regex pour supprimer les liens https qui se trouvent dans les text
s <- gsub("http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+", "", s, perl = TRUE)
s <- gsub("[\\.]*","",s,perl=TRUE)
for(k in 1:length(mots)) {
s <- gsub(mots[k], "", s, perl = TRUE)
s <- gsub("[\\W]+"," ",s,perl=TRUE)
}
return(s)
}
# vecteur de mots à supprimer
mots_supp <- c("thrones", "game", "#gameofthrones","got","#of","episode")
tweets_text <- sapply("hello @fahad @mohameed     tu va bien <>@ .. fahad .", myCleaningFunction, mots_supp)
names(tweets_text) <- NULL
tweets_text
myCleaningFunction <- function(x, mots) {
s <- x
#regex pour supprimer les liens https qui se trouvent dans les text
s <- gsub("http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+", "", s, perl = TRUE)
s <- gsub("[\\.]*","",s,perl=TRUE)
for(k in 1:length(mots)) {
s <- gsub(mots[k], "", s, perl = TRUE)
s <- gsub("[\\W]+"," ",s,perl=TRUE)
}
return(s)
}
# vecteur de mots à supprimer
mots_supp <- c("thrones", "game", "#gameofthrones","got","#of","episode")
tweets_text <- sapply("hello @fahad @mohameed     tu va bien <112424>@ .. fahad .", myCleaningFunction, mots_supp)
names(tweets_text) <- NULL
tweets_text
myCleaningFunction <- function(x, mots) {
s <- x
#regex pour supprimer les liens https qui se trouvent dans les text
s <- gsub("http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+", "", s, perl = TRUE)
s <- gsub("[\\.]*","",s,perl=TRUE)
for(k in 1:length(mots)) {
s <- gsub(mots[k], "", s, perl = TRUE)
s <- gsub("[\\W]+"," ",s,perl=TRUE)
s <- gsub("#[\\.]+"," ",s,perl=TRUE)
}
return(s)
}
# vecteur de mots à supprimer
mots_supp <- c("thrones", "game", "#gameofthrones","got","#of","episode")
tweets_text <- sapply("hello @fahad @mohameed  #ffff   tu va bien <112424>@ .. fahad .", myCleaningFunction, mots_supp)
names(tweets_text) <- NULL
tweets_text
s <- gsub("<.?*>"," ",s,perl=TRUE)
s <- gsub("<\\.?*>"," ",s,perl=TRUE)
s <- gsub("<\\.*>"," ",s,perl=TRUE)
myCleaningFunction <- function(x, mots) {
s <- x
#regex pour supprimer les liens https qui se trouvent dans les text
s <- gsub("http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+", "", s, perl = TRUE)
s <- gsub("[\\.]*","",s,perl=TRUE)
for(k in 1:length(mots)) {
s <- gsub(mots[k], "", s, perl = TRUE)
s <- gsub("[\\W]+"," ",s,perl=TRUE)
s <- gsub("<\\.*>","",s,perl=TRUE)
}
return(s)
}
# vecteur de mots à supprimer
mots_supp <- c("thrones", "game", "#gameofthrones","got","#of","episode")
tweets_text <- sapply("hello @fahad @mohameed  <ffff> gsgsg   tu va bien <112424>@ .. fahad .", myCleaningFunction, mots_supp)
names(tweets_text) <- NULL
tweets_text
s <- gsub("(<\\.*>)","",s,perl=TRUE)
myCleaningFunction <- function(x, mots) {
s <- x
#regex pour supprimer les liens https qui se trouvent dans les text
s <- gsub("http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+", "", s, perl = TRUE)
s <- gsub("[\\.]*","",s,perl=TRUE)
for(k in 1:length(mots)) {
s <- gsub(mots[k], "", s, perl = TRUE)
s <- gsub("[\\W]+"," ",s,perl=TRUE)
s <- gsub("(<\\.*>)","",s,perl=TRUE)
}
return(s)
}
# vecteur de mots à supprimer
mots_supp <- c("thrones", "game", "#gameofthrones","got","#of","episode")
tweets_text <- sapply("hello @fahad @mohameed  <ffff> gsgsg   tu va bien <112424>@ .. fahad .", myCleaningFunction, mots_supp)
names(tweets_text) <- NULL
tweets_text
myCleaningFunction <- function(x, mots) {
s <- x
#regex pour supprimer les liens https qui se trouvent dans les text
s <- gsub("http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+", "", s, perl = TRUE)
s <- gsub("[\\.]*","",s,perl=TRUE)
for(k in 1:length(mots)) {
s <- gsub(mots[k], "", s, perl = TRUE)
s <- gsub("[\\W]+"," ",s,perl=TRUE)
s <- gsub("(<\\.+>)*","",s,perl=TRUE)
}
return(s)
}
# vecteur de mots à supprimer
mots_supp <- c("thrones", "game", "#gameofthrones","got","#of","episode")
tweets_text <- sapply("hello @fahad @mohameed  <ffff>  gsgsg   tu va bien <112424>@ .. fahad .", myCleaningFunction, mots_supp)
names(tweets_text) <- NULL
tweets_text
myCleaningFunction <- function(x, mots) {
s <- x
#regex pour supprimer les liens https qui se trouvent dans les text
s <- gsub("http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+", "", s, perl = TRUE)
s <- gsub("[\\.]*","",s,perl=TRUE)
s <- gsub("[\\W]+"," ",s,perl=TRUE)
s <- gsub("(<\\.+>)*","",s,perl=TRUE)
for(k in 1:length(mots)) {
s <- gsub(mots[k], "", s, perl = TRUE)
}
return(s)
}
# vecteur de mots à supprimer
mots_supp <- c("thrones", "game", "#gameofthrones","got","#of","episode")
tweets_text <- sapply("hello @fahad @mohameed  <ffff>  gsgsg   tu va bien <112424>@ .. fahad .", myCleaningFunction, mots_supp)
names(tweets_text) <- NULL
tweets_text
myCleaningFunction <- function(x, mots) {
s <- x
#regex pour supprimer les liens https qui se trouvent dans les text
s <- gsub("http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+", "", s, perl = TRUE)
s <- gsub("[\\.]*","",s,perl=TRUE)
s <- gsub("[\\W]+"," ",s,perl=TRUE)
s <- gsub("<[\\w+]>*","",s,perl=TRUE)
for(k in 1:length(mots)) {
s <- gsub(mots[k], "", s, perl = TRUE)
}
return(s)
}
# vecteur de mots à supprimer
mots_supp <- c("thrones", "game", "#gameofthrones","got","#of","episode")
tweets_text <- sapply("hello @fahad @mohameed  <ffff>  gsgsg   tu va bien <112424>@ .. fahad .", myCleaningFunction, mots_supp)
names(tweets_text) <- NULL
tweets_text
s <- gsub("<[\\w+]>*","",s)
myCleaningFunction <- function(x, mots) {
s <- x
#regex pour supprimer les liens https qui se trouvent dans les text
s <- gsub("http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+", "", s, perl = TRUE)
s <- gsub("[\\.]*","",s,perl=TRUE)
s <- gsub("[\\W]+"," ",s,perl=TRUE)
s <- gsub("<[\\w+]>*","",s)
for(k in 1:length(mots)) {
s <- gsub(mots[k], "", s, perl = TRUE)
}
return(s)
}
# vecteur de mots à supprimer
mots_supp <- c("thrones", "game", "#gameofthrones","got","#of","episode")
tweets_text <- sapply("hello @fahad @mohameed  <ffff>  gsgsg   tu va bien <112424>@ .. fahad .", myCleaningFunction, mots_supp)
names(tweets_text) <- NULL
tweets_text
#suppression de Retweets
tweets_got = strip_retweets(tweets_got)
#cette fonction va permettre de transformer la liste de tweets extraite en un "dataframe"
got_tweets <- twListToDF(tweets_got)
str(got_tweets) #description du Dataframe
got_text<-got_tweets$text #recuperation du text des tweets
tweets_text <- stri_trans_general(got_text, "lower") # on va ici mettre tous les mots en minuscules
myCleaningFunction <- function(x, mots) {
s <- x
#regex pour supprimer les liens https qui se trouvent dans les text
s <- gsub("http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+", "", s, perl = TRUE)
#suppression des points
s <- gsub("[\\.]*","",s,perl=TRUE)
#suppression des mentions
s <- gsub("@[a-zA-Z0-9]*","",s,perl=TRUE)
s <- gsub("[\\W]+"," ",s,perl=TRUE)
#suppression de certains mots qu'on a passe en parametre
for(k in 1:length(mots)) {
s <- gsub(mots[k], "", s, perl = TRUE)
}
return(s)
}
# vecteur de mots à supprimer,on va supprimer les stopword en anglais
mots_supp <- c("brexit")
tweets_text <- sapply(tweets_text, myCleaningFunction, mots_supp)
names(tweets_text) <- NULL
donne=data.frame( id=got_tweets$id,tweet=tweets_text,
date=got_tweets$created,username=got_tweets$screenName)
View(donne) #visualisation du dataframe
write.csv2(donne, file = "brexit_tweets.csv",append = TRUE, row.names = FALSE)
write.csv2(donne, file = "brexit_tweets.csv",append = TRUE, row.names = FALSE)
#Creation d'un corpus a partir de la colonne "tweet" (le text des tweet) du dataframe "donne"
tweets_corpus <- Corpus(VectorSource(donne$tweet))
# ici cela va supprimer automatiquement tous les caractères de ponctuation
tweets_corpus <- tm_map(tweets_corpus, removePunctuation)
# ici cela va supprimer automatiquement une bonne partie des mots anglais basiques
tweets_corpus <- tm_map(tweets_corpus, removeWords, stopwords("en"))
# ici cela va supprimer automatiquement tous les espaces vides
tweets_corpus <- tm_map(tweets_corpus, stripWhitespace)
tweets_text<-tweets_corpus
dtm <- TermDocumentMatrix(tweets_corpus)
m <- as.matrix(dtm)
v <- sort(rowSums(m), decreasing = TRUE)
d <- data.frame(word = names(v),freq = v)
head(d, 20)
barplot(d[1:20,]$freq, las = 2, names.arg = d[1:20,]$word,
col ="lightblue", main ="Mots les plus fréquents",
ylab = "Fréquences")
set.seed(123456)
wordcloud(tweets_corpus, max.words = 200, colors = brewer.pal(8, "Dark2"))
library("NLP")
library("twitteR")
#le package "tm" propose un certain nombre de fonctions intéressantes pour le retravail de données textuelles
library("tm")
library("SnowballC")
library("stringi")
library("topicmodels")
library("ROAuth")
library("wordcloud")
consumer_key <- 'r1nTeZaXHwfjXKARyaZIVurc9'
consumer_secret <- 'EikApVxGDxiybnugl0hDfpPhtuloCjDO43yAJMoXlfY3BBnDya'
access_token <- '2598381610-GXnmHSk6RjqTCM3g795sSCWoU2ZobSgEDHZlEGG'
access_secret <- 'WTcf5ryAS7G0PFgWei9W06jHuYRbSKJSGPRv3knZOY0iS'
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
tweets_got <- searchTwitter("#Brexit", n=10000,lang = "en")
#suppression de Retweets
tweets_got = strip_retweets(tweets_got)
#cette fonction va permettre de transformer la liste de tweets extraite en un "dataframe"
got_tweets <- twListToDF(tweets_got)
str(got_tweets) #description du Dataframe
got_text<-got_tweets$text #recuperation du text des tweets
tweets_text <- stri_trans_general(got_text, "lower") # on va ici mettre tous les mots en minuscules
myCleaningFunction <- function(x, mots) {
s <- x
#regex pour supprimer les liens https qui se trouvent dans les text
s <- gsub("http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+", "", s, perl = TRUE)
#suppression des points
s <- gsub("[\\.]*","",s,perl=TRUE)
#suppression des mentions
s <- gsub("@[a-zA-Z0-9]*","",s,perl=TRUE)
s <- gsub("[\\W]+"," ",s,perl=TRUE)
#suppression de certains mots qu'on a passe en parametre
for(k in 1:length(mots)) {
s <- gsub(mots[k], "", s, perl = TRUE)
}
return(s)
}
# vecteur de mots à supprimer,on va supprimer les stopword en anglais
mots_supp <- c("brexit")
tweets_text <- sapply(tweets_text, myCleaningFunction, mots_supp)
names(tweets_text) <- NULL
donne=data.frame( id=got_tweets$id,tweet=tweets_text,
date=got_tweets$created,username=got_tweets$screenName)
View(donne) #visualisation du dataframe
write.csv2(donne, file = "brexit_tweets.csv",append = TRUE, row.names = FALSE)
#Creation d'un corpus a partir de la colonne "tweet" (le text des tweet) du dataframe "donne"
tweets_corpus <- Corpus(VectorSource(donne$tweet))
# ici cela va supprimer automatiquement tous les caractères de ponctuation
tweets_corpus <- tm_map(tweets_corpus, removePunctuation)
# ici cela va supprimer automatiquement une bonne partie des mots anglais basiques
tweets_corpus <- tm_map(tweets_corpus, removeWords, stopwords("en"))
# ici cela va supprimer automatiquement tous les espaces vides
tweets_corpus <- tm_map(tweets_corpus, stripWhitespace)
tweets_text<-tweets_corpus
dtm <- TermDocumentMatrix(tweets_corpus)
m <- as.matrix(dtm)
v <- sort(rowSums(m), decreasing = TRUE)
d <- data.frame(word = names(v),freq = v)
head(d, 20)
barplot(d[1:20,]$freq, las = 2, names.arg = d[1:20,]$word,
col ="lightblue", main ="Mots les plus fréquents",
ylab = "Fréquences")
set.seed(123456)
#nuage de mots
wordcloud(tweets_corpus, max.words = 200, colors = brewer.pal(8, "Dark2"))
write.csv2(got_tweets, file = "brexit.csv",append = TRUE, row.names = FALSE)
